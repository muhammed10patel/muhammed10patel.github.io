<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Open-Vocabulary Wildlife Detector from aerial images | Muhammed Patel </title> <meta name="author" content="Muhammed Patel"> <meta name="description" content="leveraging textual inputs for open vocabulary animal detection"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://muhammedpatel.com/projects/4_project/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link rel="stylesheet" href="/assets/css/photoviewer.css"> <script src="https://cdn.jsdelivr.net/npm/photoviewer/dist/photoviewer.min.js"></script> <link rel="stylesheet" href="/assets/css/table.css"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Muhammed</span> Patel </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Open-Vocabulary Wildlife Detector from aerial images</h1> <p class="post-description">leveraging textual inputs for open vocabulary animal detection</p> </header> <article> <h3 id="highlights">Highlights</h3> <ul> <li>Developed OpenWildlife (OW), an innovative open-vocabulary multi-species wildlife detector capable of identifying species across terrestrial and marine environments using natural language inputs</li> <li>The model is trained on 20 geographically diverse datasets, including birds from multiple continents, pinnipeds, cetaceans, wild animals from Africa, and domestic livestock</li> <li>Achieved state-of-the-art performance across 20 datasets with detection performance upto 0.981 mAP50 in fine-tuning setting and good generalization across 6 datasets in zero-shot setting with upto 0.812 mAP50</li> <li>Developed a novel search that uses k-nearest neighbors and breadth-first search to locate areas with high probabilities of social species presence. Our search method captures 98.7% of species while exploring only 32.3% of the total images</li> </ul> <hr> <div class="project-gallery row text-center"> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/project4/architecture_and_process_final.png" alt="Architecture overview" data-title="Architecuture of OpenWildlife" class="img-fluid rounded z-depth-1" style="width: 100%; height: auto;"> </div> </div> <div class="caption"> OpenWildlife architecture: given an aerial image and either a list of species of interest or an LLM-generated grounding caption, the model extracts image and text features. These features are fused through cross-modality enhancement, enabling language guided queries to highlight relevant regions in both modalities. Predicted animal locations can then be fed into the Social Target Search (STS) module, which prioritizes the next regions for detection based on potential social aggregations of target species. This iterative process efficiently pinpoints wildlife across large areas. </div> <h4 id="dataset">Dataset</h4> <p>We consolidated all publicly available aerial wildlife datasets for our study. Each dataset was preprocessed into the COCO annotation format with bounding boxes; datasets containing only key points were converted into fixed-size bounding boxes. The collected datasets encompass various species, including birds from multiple continents, pinnipeds, cetaceans, wild animals from Africa, and domestic livestock. Detailed descriptions of individual datasets are provided in the supplementary material.</p> <p>The 26 datasets were partitioned into two groups: 20 were used for open-set continuous pretraining, while 6 were reserved for zero-shot evaluation. This division was designed to assess the model’s adaptability across geographically-diverse contexts, such as tundras, tropical oceans, and deserts.</p> <div id="gallery1" class="project-gallery row text-center"> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> Elephants &amp; Giraffes </div> <img src="/assets/img/project4/Eikelboom_IMG_6244.JPG" alt="Image 1" data-title="Elephant &amp; Giraffe" data-alt-src="/assets/img/project4/Eikelboom_bbox_IMG_6244.JPG" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> Elephants </div> <img src="/assets/img/project4/elephant_e867581a3f3a682195014d99cbf35db3da5c4d73.jpg" alt="Image 2" data-title="Elephants" data-alt-src="/assets/img/project4/elephant_bbox_e867581a3f3a682195014d99cbf35db3da5c4d73.jpg" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> Polar Bears </div> <img src="/assets/img/project4/polar_bear_69980a43-06-03_157D300S_DSC_7892.JPG" alt="Image 4" data-title="Polar Bears" data-alt-src="/assets/img/project4/polar_bear_bbox_69980a43-06-03_157D300S_DSC_7892.JPG" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> Sea lions </div> <img src="/assets/img/project4/Seal_lion_291.jpg" alt="Image 5" data-title="Sea lions" data-alt-src="/assets/img/project4/Seal_lion_bbox_291.jpg" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> turtles </div> <img src="/assets/img/project4/turtle_20150807cr5mainnestingbeachnlane_20150808_000421_IMG_5803_NIR.jpg" alt="Image 6" data-title="turtles" data-alt-src="/assets/img/project4/turtle_bbox_20150807cr5mainnestingbeachnlane_20150808_000421_IMG_5803_NIR.jpg" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px; "> Beluga whales </div> <img src="/assets/img/project4/whale_20150818_25mm_002230.jpg" alt="Image 7" data-title="Beluga whales" data-alt-src="/assets/img/project4/whale_bbox_20150818_25mm_002230.jpg" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> Penguins </div> <img src="/assets/img/project4/penguin_cape_wallace_survey_8_451.png" alt="Image 3" data-title="Penguins" data-alt-src="/assets/img/project4/penguin_bbox_cape_wallace_survey_8_451.png" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> birds </div> <img src="/assets/img/project4/bird_Transect_A_2020_135.png" alt="Image 3" data-title="birds" data-alt-src="/assets/img/project4/bird_bbox_Transect_A_2020_135.png" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-4 col-sm-6 mt-2 position-relative"> <div style="position: absolute; top: 5px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 3px 6px; border-radius: 5px; font-size: 12px;"> livestock </div> <img src="/assets/img/project4/livestock_120.jpg" alt="Image 3" data-title="livestock" data-alt-src="/assets/img/project4/livestock_bbox_120.jpg" class="img-fluid rounded z-depth-1"> </div> </div> <div id="gallery1-container" class="text-center mt-3"> <button class="toggle-button btn btn-primary btn-sm" data-gallery="gallery1" data-toggled="false">Show Predictions</button> </div> <div class="gallery-caption text-center mt-3"> <p style="font-size: 12px; color: #555;"> The above images shows the prediction capabilities of OpenWildlife across diverse terrain and species.In some cases, the model produces additional bounding boxes, which appear valid and may highlight potential labeling errors in the dataset. Click on individual images to zoom in. Use the toggle button to switch between predictions and original images. Box color coding; TP: <span style="color: green;">Green</span>, FP: <span style="color: red;">Red</span>, FN: <span style="color: blue;">Blue</span>. </p> </div> <div class="project-gallery row text-center"> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/project3/patches.png" alt="Survey location" data-title="Patches from size 256-4096 highliting importance of the context" class="img-fluid rounded z-depth-1" style="width: 100%; height: auto;"> </div> </div> <div class="caption"> <p style="font-size: 12px; color: #555;"> Patches from size 256 to 4096. Orientation of the waves is critical to discern whales from waves. </p> </div> <h4 id="caption-generation">Caption generation</h4> <div class="project-gallery row text-center"> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/project4/rich_annotation.png" alt="Example of rich annotation" data-title="Example of rich annotation" class="img-fluid rounded z-depth-1" style="width: 40%; height: auto;"> </div> </div> <div class="caption"> <p style="font-size: 12px; color: #555;"> Example of rich annotation in general computer vision datasets. </p> </div> <p>In general computer vision there are datasets like O365, GOLD and V3Det which have rich semantic annotation for each image. An example of such rich annotation is shown in the figure below. However in aerial images of animal, obtaining such rich annotation is difficult and laborious. To circumvent that, we leverage the world knowledge acquired by LLMs. Specifically, we use OpenAI’s GPT-4o-mini, Batch API to generate a caption for our image with three components: a general description of the dataset, visual description of the animal present and details about their habitat and population density. These captions provides the necessary context that will help improve the prediction performance.</p> <p>An example of such annotation for aerial imagery is shown in below figure.</p> <div class="project-gallery row text-center"> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/project4/140803_Cam2_00456.png" alt="Example of rich annotation" data-title="Example of rich annotation" class="img-fluid rounded z-depth-1" style="width: 40%; height: auto;"> </div> </div> <div class="caption"> <p style="font-size: 12px; color: #555;"> Generated Caption: An aerial view over Cumberland Sound Bay, Nunavut, in 2014. Possible species present include beluga whales. Beluga whales are easily recognizable by their white coloration and rounded foreheads, often referred to as ”canaries of the sea” due to their vocalizations. These social species tend to form pods, with population densities often ranging from a few individuals to several dozen in a given area, especially in feeding grounds. </p> </div> <h4 id="result">Result</h4> <p>The below table shows the mAP50 performance of the OpenWildlife model on different dataset in open-set continuous fine-tuning. Meaning the model has seen this domain and species before, but the model is tested on held-out test dataset.</p> <div class="table2-container"> <table border="1" style="border-collapse: collapse; text-align: center;"> <thead> <tr> <th>Dataset</th> <th>Literature Results</th> <th colspan="2">OW Keyword</th> </tr> <tr> <th></th> <th></th> <th>Lit. Metric</th> <th>mAP50</th> </tr> </thead> <tbody> <tr> <td>Michigan</td> <td>0.73 F1 <a href="https://www.biorxiv.org/content/10.1101/2021.08.05.455311v1.full" target="_blank" rel="external nofollow noopener">[70]</a> </td> <td>0.83 F1</td> <td>0.831</td> </tr> <tr> <td>IndOcean</td> <td>0.76 F1 <a href="https://www.biorxiv.org/content/10.1101/2021.08.05.455311v1.full" target="_blank" rel="external nofollow noopener">[70]</a> </td> <td>0.81 F1</td> <td>0.845</td> </tr> <tr> <td>NewMex</td> <td>0.76 F1 <a href="https://www.biorxiv.org/content/10.1101/2021.08.05.455311v1.full" target="_blank" rel="external nofollow noopener">[70]</a> </td> <td>0.80 F1</td> <td>0.712</td> </tr> <tr> <td>Palmyra</td> <td>0.80 F1 <a href="https://www.biorxiv.org/content/10.1101/2021.08.05.455311v1.full" target="_blank" rel="external nofollow noopener">[70]</a> </td> <td>0.68 F1</td> <td>0.709</td> </tr> <tr> <td>Pfeifer</td> <td>0.66 F1 <a href="https://www.biorxiv.org/content/10.1101/2021.08.05.455311v1.full" target="_blank" rel="external nofollow noopener">[70]</a> </td> <td>0.86 F1</td> <td>0.848</td> </tr> <tr> <td>Seabird</td> <td>0.69 F1 <a href="https://www.biorxiv.org/content/10.1101/2021.08.05.455311v1.full" target="_blank" rel="external nofollow noopener">[70]</a> </td> <td>0.93 F1</td> <td>0.957</td> </tr> <tr> <td>WAfrica</td> <td>0.62 F1 <a href="https://zslpublications.onlinelibrary.wiley.com/doi/full/10.1002/rse2.200" target="_blank" rel="external nofollow noopener">[33]</a> </td> <td>0.63 F1</td> <td>0.544</td> </tr> <tr> <td>Turtle</td> <td>0.27 F1 <a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13132" target="_blank" rel="external nofollow noopener">[21]</a> </td> <td>0.38 F1</td> <td>0.247</td> </tr> <tr> <td>ArctSeal</td> <td>0.87 F1 <a href="https://hub.arcgis.com/content/bb05ab8f3b7c4ec79eca613c9273ef6f/about" target="_blank" rel="external nofollow noopener">[19]</a> </td> <td>0.87 F1</td> <td>0.921</td> </tr> <tr> <td>Han</td> <td>0.89 mAP50 <a href="https://link.springer.com/article/10.1007/s41095-019-0132-5" target="_blank" rel="external nofollow noopener">[24]</a> </td> <td>0.97 mAP50</td> <td>0.969</td> </tr> <tr> <td>WAID</td> <td>0.98 mAP50 <a href="https://www.mdpi.com/2076-3417/13/18/10397" target="_blank" rel="external nofollow noopener">[46]</a> </td> <td>0.98 mAP50</td> <td>0.981</td> </tr> <tr> <td>Kenya</td> <td>0.77 mAP30 <a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13277" target="_blank" rel="external nofollow noopener">[15]</a> </td> <td>0.86 mAP30</td> <td>0.849</td> </tr> <tr> <td>AED</td> <td>0.89 mAP † <a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/DOAI/Naude_The_Aerial_Elephant_Dataset_A_New_Public_Benchmark_for_Aerial_CVPRW_2019_paper.html" target="_blank" rel="external nofollow noopener">[47]</a> </td> <td>0.90 mAP †</td> <td>0.856</td> </tr> <tr> <td>Sea Lion</td> <td>10.86 RMSE <a href="https://www.kaggle.com/competitions/noaa-fisheries-steller-sea-lion-population-count/leaderboard" target="_blank" rel="external nofollow noopener">[29]</a> </td> <td>10.84 RMSE</td> <td>0.705</td> </tr> <tr> <td>Penguin</td> <td>39.4 RMSE <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.9903" target="_blank" rel="external nofollow noopener">[55]</a> </td> <td>365.68 RMSE</td> <td>0.404</td> </tr> <tr> <td>Izembek<a href="https://wildlife.onlinelibrary.wiley.com/doi/full/10.1002/wsb.1407" target="_blank" rel="external nofollow noopener">[71]</a> </td> <td></td> <td>-</td> <td>0.391</td> </tr> <tr> <td>DFOW14</td> <td>-</td> <td>-</td> <td>0.761</td> </tr> <tr> <td>DFOW15</td> <td>-</td> <td>-</td> <td>0.791</td> </tr> <tr> <td>DFOW16</td> <td>-</td> <td>-</td> <td>0.750</td> </tr> </tbody> </table> </div> <div class="caption"> <p style="font-size: 12px; color: #555;"> Results of open-set continuous fine-tuning. <strong>Bold</strong> numbers denote the best metric between literature results and OW Keyword results. Results marked with † indicate a 200-pixel Chebyshev distance allowance to be considered a true positive. </p> </div> <p>Since the original point of the OpenWildlife architecture is to get an openvocabulary model capable of detecting any species in any domain, we test the performance over 6 datasets in complete zero shot setting.</p> <p>The below table shows the performance in zeroshot setting:</p> <div class="table2-container"> <table> <thead> <tr> <th rowspan="2">Dataset</th> <th colspan="2">Zero-shot (mAP50)</th> <th colspan="1">Fine-tuned (mAP50)</th> </tr> <tr> <th>Keyword</th> <th>Sentence</th> <th>Keyword</th> </tr> </thead> <tbody> <tr> <td>Penguins [41]</td> <td>0.040</td> <td><strong>0.796</strong></td> <td>0.899</td> </tr> <tr> <td>DFOW17</td> <td>0.766</td> <td><strong>0.812</strong></td> <td>0.850</td> </tr> <tr> <td>DFOW23</td> <td>0.265</td> <td><strong>0.319</strong></td> <td>0.431</td> </tr> <tr> <td>Polar Bear [8]</td> <td><strong>0.498</strong></td> <td>0.297</td> <td>0.714</td> </tr> <tr> <td>SAVMAP [57]</td> <td><strong>0.071</strong></td> <td>0.002</td> <td>0.535</td> </tr> <tr> <td>Virunga-Gar. [10]</td> <td><strong>0.180</strong></td> <td>0.108</td> <td>0.718</td> </tr> </tbody> </table> </div> <h4 id="adapting-to-new-domain-active-learning">Adapting to new domain: Active learning</h4> <div class="project-gallery row text-center"> <div class="col-sm mt-3 mt-md-0"> <img src="/assets/img/project3/differences_in_new_data.png" alt="Survey location" data-title="Effective Receptive Fields (ERF) between different backbone models." class="img-fluid rounded z-depth-1" style="width: 50%; height: auto;"> </div> </div> <div class="caption"> <p style="font-size: 12px; color: #555;"> Differrences between old and new surveys. </p> </div> <p>In real world, the distribution of the dataset changes continuously. This happened with our partner, DFO in their newly captured survey. Their new survey contains images with ice, new species and different size of the whale. To address this, we use active learning which is a human-in-the-loop approach where few images are sampled from the unlabelled set such that the performance of the model will be maximized if trained on the new sample. We deploy this active learning pipeline using <a href="https://labelstud.io/" target="_blank" style="color: #007bff; text-decoration: none;" rel="external nofollow noopener">Labelstudio</a>, where model’s prediction is continuously refined by a human annotator.</p> <p>Below video shows an example image annotated and corrected using labelstudio. This workflow is moved into production and is currently utilzed by DFO for their annotation of the new survey.</p> <div style="text-align: center;"> <img src="/assets/img/project3/label_studio.gif" alt="Model Demo" style="width: 80%; border: 1px solid #ddd; border-radius: 5px;"> </div> <h4 id="active-learning-result">Active learning: Result</h4> <p>The below table shows the evolution of the model after four active learning iterations</p> <div style="display: flex; justify-content: center; margin-top: 20px;"> <table border="1" style="border-collapse: collapse; text-align: center;"> <thead style="background-color: #d3d3d3;"> <tr> <th>AL iter</th> <th>AP<sub>IoU=10</sub> </th> <th>TP</th> <th>FP</th> <th>FN</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0.601</td> <td>16</td> <td>21</td> <td>1</td> </tr> <tr> <td>2</td> <td>0.775</td> <td>24</td> <td>9</td> <td>4</td> </tr> <tr> <td>3</td> <td>0.730</td> <td>230</td> <td>83</td> <td>9</td> </tr> <tr> <td>4</td> <td>0.740</td> <td>10</td> <td>4</td> <td>5</td> </tr> <tr style="background-color: #d3d3d3;"> <td>Total</td> <td></td> <td>280</td> <td>117</td> <td>19</td> </tr> </tbody> </table> </div> <p>The before and after results of finetuning the model is visualized in the below figure.</p> <div id="gallery1" class="project-gallery row text-center"> <div class="col-md-6 col-sm-12 mt-3 position-relative"> <div style="position: absolute; top: 10px; left: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 5px; border-radius: 5px;"> Before active learning </div> <img src="/assets/img/project3/non_20230823_25mm_cam2_14127.jpg" alt="Image 1" data-title="2014 Dataset" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-6 col-sm-12 mt-3 position-relative"> <div style="position: absolute; top: 10px; right: 10px; color: white; background-color: rgba(0, 0, 0, 0.5); padding: 5px; border-radius: 5px;"> After active learning </div> <img src="/assets/img/project3/ft_20230823_25mm_cam2_14127.jpg" alt="Image 2" data-title="2015 Dataset" class="img-fluid rounded z-depth-1"> </div> </div> <div id="gallery1" class="project-gallery row text-center"> <div class="col-md-6 col-sm-12 mt-3"> <img src="/assets/img/project3/non_ft_20230828_25mm_cam1_05655.jpg" alt="Image 3" data-title="2016 Dataset" class="img-fluid rounded z-depth-1"> </div> <div class="col-md-6 col-sm-12 mt-3"> <img src="/assets/img/project3/ft_20230828_25mm_cam1_05655.jpg" alt="Image 4" data-title="2017 Dataset" class="img-fluid rounded z-depth-1"> </div> </div> <div class="gallery-caption text-center mt-3"> <p style="font-size: 12px; color: #555;"> Left shows the model’s predictions before fine-tuning. Right shows the predictions after finetuning. Box color coding: <span style="color: green;">Green</span>, FP: <span style="color: red;">Red</span>, FN: <span style="color: blue;">Blue</span>. </p> </div> <h4 id="thesis">Thesis</h4> <p>I’ve published a thesis as a part of my Master’s program and the manuscript is available <a href="https://uwspace.uwaterloo.ca/items/e249219d-797f-47cd-9166-d68cc98a5841" rel="external nofollow noopener" target="_blank">here</a>.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Muhammed Patel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?fa0110e8b42cec56ce96d912fd4bde74"></script> <script>addBackToTop();</script> <script src="/assets/js/photoviewer.js"></script> </body> </html>